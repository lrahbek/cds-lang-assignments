{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow transformers  tf-keras pandas\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa587c2664614d76a7fd834397d15bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bce6e962e343838bceece2d18fe98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625e4296e0654248a1034c2784f51d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c57e8d6972643f1a6234ee9ef84807f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1677f36ff8a243ffaed19e6d570c3897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661a7fc5492b4ffaabb88638c73da66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "classifier = pipeline(\"text-classification\", \n",
    "                      model=\"j-hartmann/emotion-english-distilroberta-base\", \n",
    "                      return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "data = pd.read_csv(\"../in/GoT-scripts/Game_of_Thrones_Script.csv\")\n",
    "#add emotion columns to copy of dataframe\n",
    "\n",
    "\n",
    "data_out[[\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"suprise\"]] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=23911, step=1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in range(len(data[\"Sentence\"])):\n",
    "    sentence = data[\"Sentence\"][sent]\n",
    "    emot_sent = classifier(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = classifier(data[\"Sentence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = pd.DataFrame.from_records(test[0]).T\n",
    "test_2.columns  = list(test_2.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label       anger\n",
       "score    0.094708\n",
       "Name: anger, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2.appe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'label': 'anger', 'score': 0.04903155937790871}</td>\n",
       "      <td>{'label': 'disgust', 'score': 0.01809948869049...</td>\n",
       "      <td>{'label': 'fear', 'score': 0.029709266498684883}</td>\n",
       "      <td>{'label': 'joy', 'score': 0.004798834677785635}</td>\n",
       "      <td>{'label': 'neutral', 'score': 0.16870249807834...</td>\n",
       "      <td>{'label': 'sadness', 'score': 0.01527708675712...</td>\n",
       "      <td>{'label': 'surprise', 'score': 0.7143812775611...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'label': 'anger', 'score': 0.019147297367453575}</td>\n",
       "      <td>{'label': 'disgust', 'score': 0.04209727421402...</td>\n",
       "      <td>{'label': 'fear', 'score': 0.005443821661174297}</td>\n",
       "      <td>{'label': 'joy', 'score': 0.005737967789173126}</td>\n",
       "      <td>{'label': 'neutral', 'score': 0.9114251732826233}</td>\n",
       "      <td>{'label': 'sadness', 'score': 0.01093045342713...</td>\n",
       "      <td>{'label': 'surprise', 'score': 0.0052179908379...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0   {'label': 'anger', 'score': 0.04903155937790871}   \n",
       "1  {'label': 'anger', 'score': 0.019147297367453575}   \n",
       "\n",
       "                                                   1  \\\n",
       "0  {'label': 'disgust', 'score': 0.01809948869049...   \n",
       "1  {'label': 'disgust', 'score': 0.04209727421402...   \n",
       "\n",
       "                                                  2  \\\n",
       "0  {'label': 'fear', 'score': 0.029709266498684883}   \n",
       "1  {'label': 'fear', 'score': 0.005443821661174297}   \n",
       "\n",
       "                                                 3  \\\n",
       "0  {'label': 'joy', 'score': 0.004798834677785635}   \n",
       "1  {'label': 'joy', 'score': 0.005737967789173126}   \n",
       "\n",
       "                                                   4  \\\n",
       "0  {'label': 'neutral', 'score': 0.16870249807834...   \n",
       "1  {'label': 'neutral', 'score': 0.9114251732826233}   \n",
       "\n",
       "                                                   5  \\\n",
       "0  {'label': 'sadness', 'score': 0.01527708675712...   \n",
       "1  {'label': 'sadness', 'score': 0.01093045342713...   \n",
       "\n",
       "                                                   6  \n",
       "0  {'label': 'surprise', 'score': 0.7143812775611...  \n",
       "1  {'label': 'surprise', 'score': 0.0052179908379...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(test_out[1:3])\n",
    "test_out = classifier(list(data[\"Sentence\"][1:5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
